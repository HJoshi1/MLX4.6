{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hugging face dataset - Done\n",
    "# Resize all images to 448 by 448 - Done\n",
    "# For each image, split into grid of 16 x 16 pixels - sequences of each patch representing the image - Done\n",
    "# Convert each patch to 1 dimensional vector - this will be the pre embedding of the patch - Done\n",
    "# Pass pre embedding to embedding layer to get the embedding of the patch \n",
    "# This will input to the encoder \n",
    "\n",
    "# Tokenise each caption in the dataset - Done\n",
    "# Create two copies of each tokenised caption, \n",
    "#  - one with start of sentence token at the start - input to decoder\n",
    "#  - one with end of sentence token at the end - label for the loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "import ast  # For converting string representation of list to list\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split up image into grid of 16 x 16 and convert to list of 1D arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patches extracted: 784\n",
      "Shape of patch vectors tensor: torch.Size([784, 768])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Path to your image\n",
    "image_path = 'flickr30k/flickr30k-images/testImage.jpg'\n",
    "\n",
    "\n",
    "def convertImageTo1DPatchesVectorList(image_path):\n",
    "    \n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Resize the image to 448 x 448 pixels\n",
    "    resized_image = image.resize((448, 448))\n",
    "\n",
    "    # Convert the image to a numpy array for easier manipulation\n",
    "    image_np = np.array(resized_image)\n",
    "\n",
    "    # Initialize a list to hold the 1D vectors for each 16x16 patch\n",
    "    patch_vectors = []\n",
    "\n",
    "    # Iterate over the image in 16x16 blocks\n",
    "    for y in range(0, image_np.shape[0], 16):\n",
    "        for x in range(0, image_np.shape[1], 16):\n",
    "            # Extract the patch\n",
    "            patch = image_np[y:y+16, x:x+16]\n",
    "\n",
    "            # Flatten the patch to a 1D vector (16*16*number_of_channels)\n",
    "            # This line assumes the image is in color (RGB), so the patch size becomes 16*16*3\n",
    "            patch_vector = patch.flatten()\n",
    "            \n",
    "            # Add the patch vector to our list\n",
    "            patch_vectors.append(patch_vector)\n",
    "\n",
    "        # Convert the list of patch vectors to a single numpy array\n",
    "    patch_vectors_np = np.array(patch_vectors)\n",
    "\n",
    "     # Convert the numpy array of patch vectors to a PyTorch tensor\n",
    "    patch_vectors_tensor = torch.tensor(patch_vectors_np, dtype=torch.float32)\n",
    "    \n",
    "    return patch_vectors_tensor\n",
    "\n",
    "patch_vectors_tensor = convertImageTo1DPatchesVectorList(image_path)\n",
    "\n",
    "print(f\"Total patches extracted: {len(patch_vectors_tensor)}\")\n",
    "# Verify the shape\n",
    "print(\"Shape of patch vectors tensor:\", patch_vectors_tensor.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentencepiece sub word encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31014\n"
     ]
    }
   ],
   "source": [
    "# Tokenise the captions \n",
    "# Create the copies (one for the decoder, the second for the loss function label)\n",
    "\n",
    "\n",
    "def encode_sentences(csv_path, sp_model_path):\n",
    "    # Load the SentencePiece model\n",
    "    sp = spm.SentencePieceProcessor(model_file=sp_model_path)\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Prepare a dictionary to hold encoded sentences\n",
    "    encoded_sentences = {'img_id': [], 'encoded_sentences': []}\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # Convert string representation of list to actual list\n",
    "        sentences = ast.literal_eval(row['raw'])\n",
    "        \n",
    "        # Encode each sentence in the list\n",
    "        encoded = [sp.encode(sentence, out_type=int) for sentence in sentences]\n",
    "        \n",
    "        # Append results\n",
    "        encoded_sentences['img_id'].append(row['img_id'])\n",
    "        encoded_sentences['encoded_sentences'].append(encoded)\n",
    "    \n",
    "    # Convert dictionary to DataFrame for easy handling/viewing\n",
    "    encoded_df = pd.DataFrame(encoded_sentences)\n",
    "    return encoded_df\n",
    "\n",
    "# Usage example\n",
    "csv_path = 'flickr_annotations_30k.csv'\n",
    "sp_model_path = 'spm.model'\n",
    "encoded_captions = encode_sentences(csv_path, sp_model_path) #Dimensions - batch size x num_images x num_captions x max caption length (after padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[19, 29, 373, 14, 2280, 124, 195, 20, 74, 177, 30, 360, 83, 7, 8, 538, 5],\n",
       " [19, 29, 15, 1113, 792, 17, 62, 95, 367, 1554, 5],\n",
       " [19, 40, 7, 55, 281, 17, 37, 7, 4, 538, 5],\n",
       " [6, 12, 7, 4, 32, 25, 37, 7, 4, 805, 5],\n",
       " [19, 555, 573, 653, 3793, 1927, 157, 5]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_captions.encoded_sentences[0] #encoding for 5 captions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_length):\n",
    "        super().__init__()\n",
    "        self.pos_encoding = self.create_pos_encoding(d_model, max_length)\n",
    "\n",
    "    def create_pos_encoding(self, d_model, max_length):\n",
    "        position = torch.arange(max_length).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_length, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        return nn.Parameter(pe, requires_grad=False)\n",
    "    \n",
    "    def forward(self, embeddings):\n",
    "        # Use embeddings to determine sequence length and to get device information\n",
    "        seq_len = embeddings.size(1)\n",
    "        pos_encoding = self.pos_encoding[:, :seq_len, :].to(embeddings.device)\n",
    "        print(pos_encoding.shape)\n",
    "        print(embeddings.shape)\n",
    "        return embeddings + pos_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedImageEncoder(nn.Module):\n",
    "    def __init__(self, num_patches, embed_dim=768, num_layers=6, num_heads=12, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.embed_dim = embed_dim\n",
    "        self.positional_encoding = PositionalEncoding(embed_dim, num_patches)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=d_ff, dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, flat_patches):\n",
    "        # Assume flat_patches is of shape [B, num_patches, embed_dim]\n",
    "        # where each patch is already flattened into a vector of embed_dim\n",
    "        x = flat_patches\n",
    "        \n",
    "        # You might need to adjust positional encodings if the sequence length\n",
    "        # could vary or if it's dynamically determined\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.transformer_encoder(x)  # Pass through transformer encoder\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA (NVIDIA GPU) is available, then use it; otherwise, fall back to CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# For PyTorch 1.12 or newer, to include support for Apple Silicon (MPS)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')  # Use Apple's Metal Performance Shaders (MPS)\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')  # Use NVIDIA GPU with CUDA\n",
    "else:\n",
    "    device = torch.device('cpu')  # Use CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Image Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded patches shape: torch.Size([1, 784, 768])\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "num_patches = (448 // 16) * (448 // 16)  # Assuming the image is resized to 448x448 and patches are 16x16\n",
    "embed_dim = 768  # Assuming each patch is projected to an embedding dimension of 768\n",
    "\n",
    "# Initialize the encoder\n",
    "image_encoder = ModifiedImageEncoder(\n",
    "    num_patches=num_patches,\n",
    "    embed_dim=embed_dim,\n",
    "    num_layers=6,\n",
    "    num_heads=12,\n",
    "    d_ff=2048,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Assuming the device setup from the previous discussion\n",
    "image_encoder = image_encoder.to(device)\n",
    "patch_vectors_tensor = patch_vectors_tensor.to(device)\n",
    "\n",
    "# Encode the preprocessed patches\n",
    "with torch.no_grad():\n",
    "    encoded_patches = image_encoder(patch_vectors_tensor.unsqueeze(0))  # Add a batch dimension\n",
    "\n",
    "print(\"Encoded patches shape:\", encoded_patches.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, d_ff, max_length, dropout=0.1):\n",
    "        super(ImageCaptioningDecoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.positional_encoding = PositionalEncoding(embed_dim, max_length)\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=d_ff, dropout=dropout, batch_first=True)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_tokens, encoder_features, tgt_mask=None):\n",
    "        # tgt_mask is used to prevent the decoder from peeking at future tokens\n",
    "          # Convert token IDs to embeddings\n",
    "        input_embeddings = self.embedding(input_tokens)  # This should fix the dimension issue\n",
    "        \n",
    "        # Add positional encoding\n",
    "        input_embeddings = input_embeddings + self.positional_encoding(input_embeddings)\n",
    "    \n",
    "        # tgt_embeddings = self.embedding(input_tokens) + self.positional_encoding(input_tokens)\n",
    "        tgt_embeddings = input_embeddings #Renaming as part of debugging - nothing important happening here \n",
    "        \n",
    "        decoder_output = self.transformer_decoder(tgt=tgt_embeddings, memory=encoder_features, tgt_mask=tgt_mask)\n",
    "        output = self.fc_out(decoder_output)\n",
    "        return output\n",
    "    \n",
    "    def subsequent_mask(size):\n",
    "        \"Mask out subsequent positions.\"\n",
    "        attn_shape = (1, size, size)\n",
    "        subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(torch.bool)\n",
    "        return subsequent_mask == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 768])\n",
      "torch.Size([4, 10])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (768) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/harshil/Documents/MLX4/MLX4.6-MultiModalTransformers-CaptionsFromImages/MLX4.6/preproccessor.ipynb Cell 20\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshil/Documents/MLX4/MLX4.6-MultiModalTransformers-CaptionsFromImages/MLX4.6/preproccessor.ipynb#X35sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m tgt_mask \u001b[39m=\u001b[39m tgt_mask\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshil/Documents/MLX4/MLX4.6-MultiModalTransformers-CaptionsFromImages/MLX4.6/preproccessor.ipynb#X35sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# Run the decoder\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshil/Documents/MLX4/MLX4.6-MultiModalTransformers-CaptionsFromImages/MLX4.6/preproccessor.ipynb#X35sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# Note: Omit tgt_mask if testing for inference\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/harshil/Documents/MLX4/MLX4.6-MultiModalTransformers-CaptionsFromImages/MLX4.6/preproccessor.ipynb#X35sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m output \u001b[39m=\u001b[39m decoder(tokenized_captions, encoded_image_features, tgt_mask\u001b[39m=\u001b[39;49mtgt_mask)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshil/Documents/MLX4/MLX4.6-MultiModalTransformers-CaptionsFromImages/MLX4.6/preproccessor.ipynb#X35sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mOutput shape:\u001b[39m\u001b[39m\"\u001b[39m, output\u001b[39m.\u001b[39mshape)  \u001b[39m# Expected shape: [batch_size, seq_length, vocab_size]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/week3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/week3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/harshil/Documents/MLX4/MLX4.6-MultiModalTransformers-CaptionsFromImages/MLX4.6/preproccessor.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshil/Documents/MLX4/MLX4.6-MultiModalTransformers-CaptionsFromImages/MLX4.6/preproccessor.ipynb#X35sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_tokens, encoder_features, tgt_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshil/Documents/MLX4/MLX4.6-MultiModalTransformers-CaptionsFromImages/MLX4.6/preproccessor.ipynb#X35sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m# tgt_mask is used to prevent the decoder from peeking at future tokens\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/harshil/Documents/MLX4/MLX4.6-MultiModalTransformers-CaptionsFromImages/MLX4.6/preproccessor.ipynb#X35sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     tgt_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(input_tokens) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpositional_encoding(input_tokens)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshil/Documents/MLX4/MLX4.6-MultiModalTransformers-CaptionsFromImages/MLX4.6/preproccessor.ipynb#X35sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     decoder_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_decoder(tgt\u001b[39m=\u001b[39mtgt_embeddings, memory\u001b[39m=\u001b[39mencoder_features, tgt_mask\u001b[39m=\u001b[39mtgt_mask)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshil/Documents/MLX4/MLX4.6-MultiModalTransformers-CaptionsFromImages/MLX4.6/preproccessor.ipynb#X35sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_out(decoder_output)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/week3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/week3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/harshil/Documents/MLX4/MLX4.6-MultiModalTransformers-CaptionsFromImages/MLX4.6/preproccessor.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshil/Documents/MLX4/MLX4.6-MultiModalTransformers-CaptionsFromImages/MLX4.6/preproccessor.ipynb#X35sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(pos_encoding\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshil/Documents/MLX4/MLX4.6-MultiModalTransformers-CaptionsFromImages/MLX4.6/preproccessor.ipynb#X35sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(embeddings\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/harshil/Documents/MLX4/MLX4.6-MultiModalTransformers-CaptionsFromImages/MLX4.6/preproccessor.ipynb#X35sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mreturn\u001b[39;00m embeddings \u001b[39m+\u001b[39;49m pos_encoding\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (768) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "# Mock data for demonstration purposes\n",
    "batch_size = 4  # Number of samples in your batch\n",
    "seq_length = 10  # Length of tokenized captions\n",
    "encoded_dim = 768  # Dimension of encoder output features\n",
    "num_patches = 784  # Assuming this from the encoder\n",
    "vocab_size = 32000  # Vocabulary size from the SentencePiece model\n",
    "\n",
    "# Mock tensor representing encoded image features from the encoder\n",
    "# Shape: [batch_size, num_patches, encoded_dim]\n",
    "encoded_image_features = torch.rand(batch_size, num_patches, encoded_dim)\n",
    "\n",
    "# Mock tensor representing tokenized captions\n",
    "# Shape: [batch_size, seq_length]\n",
    "tokenized_captions = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
    "\n",
    "decoder = ImageCaptioningDecoder(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=encoded_dim,  # Matching the dimension of the encoder's output\n",
    "    num_heads=8,\n",
    "    num_layers=6,\n",
    "    d_ff=2048,\n",
    "    max_length=seq_length,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "def generate_subsequent_mask(seq_length):\n",
    "    mask = torch.triu(torch.ones((seq_length, seq_length)) * float('-inf'), diagonal=1)\n",
    "    return mask\n",
    "\n",
    "tgt_mask = generate_subsequent_mask(seq_length)\n",
    "\n",
    "# Ensure everything is on the same device, for example, CPU for simplicity\n",
    "decoder.to(\"cpu\")\n",
    "encoded_image_features = encoded_image_features.to(\"cpu\")\n",
    "tokenized_captions = tokenized_captions.to(\"cpu\")\n",
    "tgt_mask = tgt_mask.to(\"cpu\")\n",
    "\n",
    "# Run the decoder\n",
    "# Note: Omit tgt_mask if testing for inference\n",
    "output = decoder(tokenized_captions, encoded_image_features, tgt_mask=tgt_mask)\n",
    "\n",
    "print(\"Output shape:\", output.shape)  # Expected shape: [batch_size, seq_length, vocab_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
