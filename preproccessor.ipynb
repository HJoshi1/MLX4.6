{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hugging face dataset - Done\n",
    "# Resize all images to 448 by 448 - Done\n",
    "# For each image, split into grid of 16 x 16 pixels - sequences of each patch representing the image - Done\n",
    "# Convert each patch to 1 dimensional vector - this will be the pre embedding of the patch - Done\n",
    "# Pass pre embedding to embedding layer to get the embedding of the patch \n",
    "# This will input to the encoder \n",
    "\n",
    "# Tokenise each caption in the dataset - Done\n",
    "# Create two copies of each tokenised caption, \n",
    "#  - one with start of sentence token at the start - input to decoder\n",
    "#  - one with end of sentence token at the end - label for the loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "import ast  # For converting string representation of list to list\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split up image into grid of 16 x 16 and convert to list of 1D arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patches extracted: 784\n",
      "Shape of patch vectors tensor: torch.Size([784, 768])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Path to your image\n",
    "image_path = 'flickr30k/flickr30k-images/testImage.jpg'\n",
    "\n",
    "\n",
    "def convertImageTo1DPatchesVectorList(image_path):\n",
    "    \n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Resize the image to 448 x 448 pixels\n",
    "    resized_image = image.resize((448, 448))\n",
    "\n",
    "    # Convert the image to a numpy array for easier manipulation\n",
    "    image_np = np.array(resized_image)\n",
    "\n",
    "    # Initialize a list to hold the 1D vectors for each 16x16 patch\n",
    "    patch_vectors = []\n",
    "\n",
    "    # Iterate over the image in 16x16 blocks\n",
    "    for y in range(0, image_np.shape[0], 16):\n",
    "        for x in range(0, image_np.shape[1], 16):\n",
    "            # Extract the patch\n",
    "            patch = image_np[y:y+16, x:x+16]\n",
    "\n",
    "            # Flatten the patch to a 1D vector (16*16*number_of_channels)\n",
    "            # This line assumes the image is in color (RGB), so the patch size becomes 16*16*3\n",
    "            patch_vector = patch.flatten()\n",
    "            \n",
    "            # Add the patch vector to our list\n",
    "            patch_vectors.append(patch_vector)\n",
    "\n",
    "        # Convert the list of patch vectors to a single numpy array\n",
    "    patch_vectors_np = np.array(patch_vectors)\n",
    "\n",
    "     # Convert the numpy array of patch vectors to a PyTorch tensor\n",
    "    patch_vectors_tensor = torch.tensor(patch_vectors_np, dtype=torch.float32)\n",
    "    \n",
    "    return patch_vectors_tensor\n",
    "\n",
    "image_patch_vectors_tensor = convertImageTo1DPatchesVectorList(image_path)\n",
    "\n",
    "print(f\"Total patches extracted: {len(image_patch_vectors_tensor)}\")\n",
    "# Verify the shape\n",
    "print(\"Shape of patch vectors tensor:\", image_patch_vectors_tensor.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentencepiece sub word encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenise the captions \n",
    "# Create the copies (one for the decoder, the second for the loss function label)\n",
    "\n",
    "\n",
    "def encode_sentences(csv_path, sp_model_path):\n",
    "    # Load the SentencePiece model\n",
    "    sp = spm.SentencePieceProcessor(model_file=sp_model_path)\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Prepare a dictionary to hold encoded sentences\n",
    "    encoded_sentences = {'img_id': [], 'encoded_sentences': []}\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # Convert string representation of list to actual list\n",
    "        sentences = ast.literal_eval(row['raw'])\n",
    "        \n",
    "        # Encode each sentence in the list\n",
    "        encoded = [sp.encode(sentence, out_type=int) for sentence in sentences]\n",
    "        \n",
    "        # Append results\n",
    "        encoded_sentences['img_id'].append(row['img_id'])\n",
    "        encoded_sentences['encoded_sentences'].append(encoded)\n",
    "    \n",
    "    # Convert dictionary to DataFrame for easy handling/viewing\n",
    "    encoded_df = pd.DataFrame(encoded_sentences)\n",
    "    return encoded_df\n",
    "\n",
    "# Usage example\n",
    "csv_path = 'flickr_annotations_30k.csv'\n",
    "sp_model_path = 'spm.model'\n",
    "encoded_captions = encode_sentences(csv_path, sp_model_path) #Dimensions - batch size x num_images x num_captions x max caption length (after padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[19, 29, 373, 14, 2280, 124, 195, 20, 74, 177, 30, 360, 83, 7, 8, 538, 5],\n",
       " [19, 29, 15, 1113, 792, 17, 62, 95, 367, 1554, 5],\n",
       " [19, 40, 7, 55, 281, 17, 37, 7, 4, 538, 5],\n",
       " [6, 12, 7, 4, 32, 25, 37, 7, 4, 805, 5],\n",
       " [19, 555, 573, 653, 3793, 1927, 157, 5]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_captions.encoded_sentences[0] #encoding for 5 captions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_length):\n",
    "        super().__init__()\n",
    "        self.pos_encoding = self.create_pos_encoding(d_model, max_length)\n",
    "\n",
    "    def create_pos_encoding(self, d_model, max_length):\n",
    "        position = torch.arange(max_length).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_length, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        return nn.Parameter(pe, requires_grad=False)\n",
    "    \n",
    "    def forward(self, embeddings):\n",
    "        # Use embeddings to determine sequence length and to get device information\n",
    "        seq_len = embeddings.size(1)\n",
    "        pos_encoding = self.pos_encoding[:, :seq_len, :].to(embeddings.device)\n",
    "        # print(pos_encoding.shape)\n",
    "        # print(embeddings.shape)\n",
    "        return embeddings + pos_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedImageEncoder(nn.Module):\n",
    "    def __init__(self, num_patches, embed_dim=768, num_layers=6, num_heads=12, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.embed_dim = embed_dim\n",
    "        self.positional_encoding = PositionalEncoding(embed_dim, num_patches)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=d_ff, dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, flat_patches):\n",
    "        # Assume flat_patches is of shape [B, num_patches, embed_dim]\n",
    "        # where each patch is already flattened into a vector of embed_dim\n",
    "        x = flat_patches\n",
    "        \n",
    "        # You might need to adjust positional encodings if the sequence length\n",
    "        # could vary or if it's dynamically determined\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.transformer_encoder(x)  # Pass through transformer encoder\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA (NVIDIA GPU) is available, then use it; otherwise, fall back to CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# For PyTorch 1.12 or newer, to include support for Apple Silicon (MPS)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')  # Use Apple's Metal Performance Shaders (MPS)\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')  # Use NVIDIA GPU with CUDA\n",
    "else:\n",
    "    device = torch.device('cpu')  # Use CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Image Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded patches shape: torch.Size([1, 784, 768])\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "num_patches = (448 // 16) * (448 // 16)  # Assuming the image is resized to 448x448 and patches are 16x16\n",
    "embed_dim = 768  # Assuming each patch is projected to an embedding dimension of 768\n",
    "\n",
    "# Initialize the encoder\n",
    "image_encoder = ModifiedImageEncoder(\n",
    "    num_patches=num_patches,\n",
    "    embed_dim=embed_dim,\n",
    "    num_layers=6,\n",
    "    num_heads=12,\n",
    "    d_ff=2048,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Assuming the device setup from the previous discussion\n",
    "image_encoder = image_encoder.to(device)\n",
    "image_patch_vectors_tensor = image_patch_vectors_tensor.to(device)\n",
    "\n",
    "# Encode the preprocessed patches\n",
    "with torch.no_grad():\n",
    "    encoded_patches = image_encoder(image_patch_vectors_tensor.unsqueeze(0))  # Add a batch dimension\n",
    "\n",
    "print(\"Encoded patches shape:\", encoded_patches.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, d_ff, max_length, dropout=0.1):\n",
    "        super(ImageCaptioningDecoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.positional_encoding = PositionalEncoding(embed_dim, max_length)\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=d_ff, dropout=dropout, batch_first=True)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_tokens, encoder_features, tgt_mask=None):\n",
    "        # tgt_mask is used to prevent the decoder from peeking at future tokens\n",
    "          # Convert token IDs to embeddings\n",
    "        input_embeddings = self.embedding(input_tokens)  # This should fix the dimension issue\n",
    "        \n",
    "        # Add positional encoding\n",
    "        input_embeddings = input_embeddings + self.positional_encoding(input_embeddings)\n",
    "    \n",
    "        # tgt_embeddings = self.embedding(input_tokens) + self.positional_encoding(input_tokens)\n",
    "        tgt_embeddings = input_embeddings #Renaming as part of debugging - nothing important happening here \n",
    "        \n",
    "        decoder_output = self.transformer_decoder(tgt=tgt_embeddings, memory=encoder_features, tgt_mask=tgt_mask)\n",
    "        output = self.fc_out(decoder_output)\n",
    "        return output\n",
    "    \n",
    "    def subsequent_mask(size):\n",
    "        \"Mask out subsequent positions.\"\n",
    "        attn_shape = (1, size, size)\n",
    "        subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(torch.bool)\n",
    "        return subsequent_mask == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([4, 10, 32000])\n"
     ]
    }
   ],
   "source": [
    "# Mock data for demonstration purposes\n",
    "batch_size = 4  # Number of samples in your batch\n",
    "seq_length = 10  # Length of tokenized captions\n",
    "encoded_dim = 768  # Dimension of encoder output features\n",
    "num_patches = 784  # Assuming this from the encoder\n",
    "vocab_size = 32000  # Vocabulary size from the SentencePiece model\n",
    "\n",
    "# Mock tensor representing encoded image features from the encoder\n",
    "# Shape: [batch_size, num_patches, encoded_dim]\n",
    "encoded_image_features = torch.rand(batch_size, num_patches, encoded_dim)\n",
    "\n",
    "# Mock tensor representing tokenized captions\n",
    "# Shape: [batch_size, seq_length]\n",
    "tokenized_captions = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
    "\n",
    "decoder = ImageCaptioningDecoder(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=encoded_dim,  # Matching the dimension of the encoder's output\n",
    "    num_heads=8,\n",
    "    num_layers=6,\n",
    "    d_ff=2048,\n",
    "    max_length=seq_length,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "def generate_subsequent_mask(seq_length):\n",
    "    mask = torch.triu(torch.ones((seq_length, seq_length)) * float('-inf'), diagonal=1)\n",
    "    return mask\n",
    "\n",
    "tgt_mask = generate_subsequent_mask(seq_length)\n",
    "\n",
    "# Ensure everything is on the same device, for example, CPU for simplicity\n",
    "decoder.to(\"cpu\")\n",
    "encoded_image_features = encoded_image_features.to(\"cpu\")\n",
    "tokenized_captions = tokenized_captions.to(\"cpu\")\n",
    "tgt_mask = tgt_mask.to(\"cpu\")\n",
    "\n",
    "# Run the decoder\n",
    "# Note: Omit tgt_mask if testing for inference\n",
    "output = decoder(tokenized_captions, encoded_image_features, tgt_mask=tgt_mask)\n",
    "\n",
    "print(\"Output shape:\", output.shape)  # Expected shape: [batch_size, seq_length, vocab_size]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Captioning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder, vocab_size, start_token_id, end_token_id, max_length=50):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.start_token_id = start_token_id\n",
    "        self.end_token_id = end_token_id\n",
    "        self.max_length = max_length\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, images, captions=None, tgt_mask=None):\n",
    "        encoder_features = self.encoder(images)\n",
    "\n",
    "        # During training, captions are provided for teacher forcing\n",
    "        if captions is not None:\n",
    "            return self.decoder(captions, encoder_features, tgt_mask=tgt_mask)\n",
    "        else:\n",
    "            # During inference, generate captions autoregressively\n",
    "            return self.generate_caption(encoder_features)\n",
    "\n",
    "    def generate_caption(self, encoder_features):\n",
    "        batch_size = encoder_features.size(0)\n",
    "        input_tokens = torch.full((batch_size, 1), self.start_token_id, dtype=torch.long, device=encoder_features.device)\n",
    "\n",
    "        for _ in range(self.max_length):\n",
    "            output = self.decoder(input_tokens, encoder_features)\n",
    "            next_token = output.argmax(-1)[:, -1].unsqueeze(1)  # Pick the most likely next token\n",
    "            input_tokens = torch.cat([input_tokens, next_token], dim=1)  # Append to sequence\n",
    "            if torch.all(next_token == self.end_token_id):\n",
    "                break\n",
    "\n",
    "        return input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageCaptioningModel(\n",
       "  (encoder): ModifiedImageEncoder(\n",
       "    (positional_encoding): PositionalEncoding()\n",
       "    (transformer_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): ImageCaptioningDecoder(\n",
       "    (positional_encoding): PositionalEncoding()\n",
       "    (embedding): Embedding(16000, 768)\n",
       "    (transformer_decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=768, out_features=16000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 16_000\n",
    "\n",
    "# Initialize the encoder and decoder with appropriate configurations\n",
    "encoder = ModifiedImageEncoder(\n",
    "    num_patches=784,  # Assuming images are processed into 784 patches\n",
    "    embed_dim=768,    # Dimensionality of the patch embeddings\n",
    "    num_layers=6,     # Number of transformer layers in the encoder\n",
    "    num_heads=12,     # Number of attention heads in each transformer layer\n",
    "    d_ff=2048,        # Dimensionality of the feedforward network in each transformer layer\n",
    "    dropout=0.1       # Dropout rate\n",
    ")\n",
    "\n",
    "decoder = ImageCaptioningDecoder(\n",
    "    vocab_size=vocab_size,  # Adjust based on your SentencePiece model\n",
    "    embed_dim=768,\n",
    "    num_heads=8,\n",
    "    num_layers=6,\n",
    "    d_ff=2048,\n",
    "    max_length=50,  # Adjust based on the maximum sequence length you want to support\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "image_captioning_model = ImageCaptioningModel(encoder, decoder, vocab_size=32000, start_token_id=1, end_token_id=2)\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "image_captioning_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "torch.Size([5, 17, 16000])\n",
      "torch.Size([80, 16000])\n",
      "torch.Size([80])\n",
      "------\n",
      "Epoch [1/10], Loss: 9.8922\n",
      "------\n",
      "torch.Size([5, 17, 16000])\n",
      "torch.Size([80, 16000])\n",
      "torch.Size([80])\n",
      "------\n",
      "Epoch [2/10], Loss: 8.3831\n",
      "------\n",
      "torch.Size([5, 17, 16000])\n",
      "torch.Size([80, 16000])\n",
      "torch.Size([80])\n",
      "------\n",
      "Epoch [3/10], Loss: 7.6796\n",
      "------\n",
      "torch.Size([5, 17, 16000])\n",
      "torch.Size([80, 16000])\n",
      "torch.Size([80])\n",
      "------\n",
      "Epoch [4/10], Loss: 7.2637\n",
      "------\n",
      "torch.Size([5, 17, 16000])\n",
      "torch.Size([80, 16000])\n",
      "torch.Size([80])\n",
      "------\n",
      "Epoch [5/10], Loss: 6.9855\n",
      "------\n",
      "torch.Size([5, 17, 16000])\n",
      "torch.Size([80, 16000])\n",
      "torch.Size([80])\n",
      "------\n",
      "Epoch [6/10], Loss: 6.7690\n",
      "------\n",
      "torch.Size([5, 17, 16000])\n",
      "torch.Size([80, 16000])\n",
      "torch.Size([80])\n",
      "------\n",
      "Epoch [7/10], Loss: 6.5707\n",
      "------\n",
      "torch.Size([5, 17, 16000])\n",
      "torch.Size([80, 16000])\n",
      "torch.Size([80])\n",
      "------\n",
      "Epoch [8/10], Loss: 6.3610\n",
      "------\n",
      "torch.Size([5, 17, 16000])\n",
      "torch.Size([80, 16000])\n",
      "torch.Size([80])\n",
      "------\n",
      "Epoch [9/10], Loss: 6.1621\n",
      "------\n",
      "torch.Size([5, 17, 16000])\n",
      "torch.Size([80, 16000])\n",
      "torch.Size([80])\n",
      "------\n",
      "Epoch [10/10], Loss: 5.8471\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# adjust to use a scheduler - 0.01 for the first 20 epochs, then 0.001 \n",
    "\n",
    "# Configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Assuming these are already defined\n",
    "# image_captioning_model: your unified image captioning model\n",
    "# images: a tensor containing all your training images\n",
    "# captions: a tensor containing all corresponding tokenized captions for these images\n",
    "\n",
    "images = image_patch_vectors_tensor.unsqueeze(0).repeat(5, 1, 1)\n",
    "# captions = torch.tensor(encoded_captions.encoded_sentences[0], dtype=torch.long)\n",
    "caption_tensors = [torch.tensor(c, dtype=torch.long) for c in encoded_captions.encoded_sentences[0]]\n",
    "captions_padded = pad_sequence(caption_tensors, batch_first=True, padding_value=0)\n",
    "\n",
    "\n",
    "image_captioning_model.to(device)\n",
    "images = images.to(device)\n",
    "captions = captions_padded.to(device)\n",
    "\n",
    "# Prepare input captions for the decoder: shift captions to the right with start_token at the beginning\n",
    "start_token_id = 1  # Adjust based on your setup\n",
    "# decoder_input = torch.cat([torch.full((1, captions.size(1)), start_token_id, device=device), captions[:, :-1]], dim=0)\n",
    "decoder_input = torch.cat([\n",
    "    torch.full((captions_padded.size(0), 1), start_token_id, device=device),\n",
    "    captions_padded[:, :-1]\n",
    "], dim=1)# Corrected dimension for concatenation\n",
    "\n",
    "optimizer = optim.Adam(image_captioning_model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0)  # Assuming token ID 0 is used for padding\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    image_captioning_model.train()  # Set model to training mode\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = image_captioning_model(images, decoder_input)\n",
    "    # adjusted_outputs = outputs[:, :-1, :].reshape(-1, vocab_size)  # This reshapes outputs considering exclusion\n",
    "    corrected_outputs = outputs[:, start_token_id:, :].reshape(-1, vocab_size)  # Reshape to [80 (5*16), 16000]\n",
    "\n",
    "\n",
    "    # Compute the loss\n",
    "    targets = captions_padded[:, start_token_id:].reshape(-1)  # Exclude the start token from targets\n",
    "    loss = loss_fn(corrected_outputs, targets)\n",
    "\n",
    "    # Backward pass and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(image_captioning_model.state_dict(), 'image_captioning_model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated caption: ...................\n"
     ]
    }
   ],
   "source": [
    "image_input = image_patch_vectors_tensor.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "decoder_input = torch.tensor([[start_token_id]], dtype=torch.long).to(device)  # Shape: [1, 1]\n",
    "\n",
    "eos_token_id = 2  # Ensure this matches the EOS token ID used during training\n",
    "max_length = 20  # Maximum length of the generated caption\n",
    "\n",
    "while len(decoder_input[0]) < max_length:\n",
    "    with torch.no_grad():  # No need to track gradients during inference\n",
    "        outputs = image_captioning_model(image_input, decoder_input)\n",
    "        # Get the last token in the sequence's predictions\n",
    "        last_token_logits = outputs[:, -1, :]\n",
    "        # Choose the most likely next token ID\n",
    "        predicted_token_id = last_token_logits.argmax(1).item()\n",
    "        \n",
    "        # Append predicted token ID to the decoder input\n",
    "        decoder_input = torch.cat([decoder_input, torch.tensor([[predicted_token_id]], dtype=torch.long).to(device)], dim=1)\n",
    "        \n",
    "        # Break the loop if EOS token is generated\n",
    "        if predicted_token_id == eos_token_id:\n",
    "            break\n",
    "\n",
    "# Remove the initial SOS token and convert the token IDs back to text\n",
    "generated_caption_ids = decoder_input[0][1:].tolist()  # Exclude the SOS token for display\n",
    "\n",
    "# Assuming you have a function or a tokenizer object to convert IDs to text\n",
    "sp = spm.SentencePieceProcessor(model_file=sp_model_path)\n",
    "    \n",
    "generated_caption = sp.decode(generated_caption_ids)\n",
    "print(\"Generated caption:\", generated_caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
