{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hugging face dataset - Done\n",
    "# Resize all images to 448 by 448 - Done\n",
    "# For each image, split into grid of 16 x 16 pixels - sequences of each patch representing the image - Done\n",
    "# Convert each patch to 1 dimensional vector - this will be the pre embedding of the patch - Done\n",
    "# Pass pre embedding to embedding layer to get the embedding of the patch \n",
    "# This will input to the encoder \n",
    "\n",
    "# Tokenise each caption in the dataset - Done\n",
    "# Create two copies of each tokenised caption, \n",
    "#  - one with start of sentence token at the start - input to decoder\n",
    "#  - one with end of sentence token at the end - label for the loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "import ast  # For converting string representation of list to list\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split up image into grid of 16 x 16 and convert to list of 1D arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patches extracted: 784\n",
      "Shape of patch vectors tensor: torch.Size([784, 768])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Path to your image\n",
    "image_path = 'flickr30k/flickr30k-images/testImage.jpg'\n",
    "\n",
    "\n",
    "def convertImageTo1DPatchesVectorList(image_path):\n",
    "    \n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Resize the image to 448 x 448 pixels\n",
    "    resized_image = image.resize((448, 448))\n",
    "\n",
    "    # Convert the image to a numpy array for easier manipulation\n",
    "    image_np = np.array(resized_image)\n",
    "\n",
    "    # Initialize a list to hold the 1D vectors for each 16x16 patch\n",
    "    patch_vectors = []\n",
    "\n",
    "    # Iterate over the image in 16x16 blocks\n",
    "    for y in range(0, image_np.shape[0], 16):\n",
    "        for x in range(0, image_np.shape[1], 16):\n",
    "            # Extract the patch\n",
    "            patch = image_np[y:y+16, x:x+16]\n",
    "\n",
    "            # Flatten the patch to a 1D vector (16*16*number_of_channels)\n",
    "            # This line assumes the image is in color (RGB), so the patch size becomes 16*16*3\n",
    "            patch_vector = patch.flatten()\n",
    "            \n",
    "            # Add the patch vector to our list\n",
    "            patch_vectors.append(patch_vector)\n",
    "\n",
    "        # Convert the list of patch vectors to a single numpy array\n",
    "    patch_vectors_np = np.array(patch_vectors)\n",
    "\n",
    "     # Convert the numpy array of patch vectors to a PyTorch tensor\n",
    "    patch_vectors_tensor = torch.tensor(patch_vectors_np, dtype=torch.float32)\n",
    "    \n",
    "    return patch_vectors_tensor\n",
    "\n",
    "patch_vectors_tensor = convertImageTo1DPatchesVectorList(image_path)\n",
    "\n",
    "print(f\"Total patches extracted: {len(patch_vectors_tensor)}\")\n",
    "# Verify the shape\n",
    "print(\"Shape of patch vectors tensor:\", patch_vectors_tensor.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentencepiece sub word encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31014\n"
     ]
    }
   ],
   "source": [
    "# Tokenise the captions \n",
    "# Create the copies (one for the decoder, the second for the loss function label)\n",
    "\n",
    "\n",
    "def encode_sentences(csv_path, sp_model_path):\n",
    "    # Load the SentencePiece model\n",
    "    sp = spm.SentencePieceProcessor(model_file=sp_model_path)\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Prepare a dictionary to hold encoded sentences\n",
    "    encoded_sentences = {'img_id': [], 'encoded_sentences': []}\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # Convert string representation of list to actual list\n",
    "        sentences = ast.literal_eval(row['raw'])\n",
    "        \n",
    "        # Encode each sentence in the list\n",
    "        encoded = [sp.encode(sentence, out_type=int) for sentence in sentences]\n",
    "        \n",
    "        # Append results\n",
    "        encoded_sentences['img_id'].append(row['img_id'])\n",
    "        encoded_sentences['encoded_sentences'].append(encoded)\n",
    "    \n",
    "    # Convert dictionary to DataFrame for easy handling/viewing\n",
    "    encoded_df = pd.DataFrame(encoded_sentences)\n",
    "    return encoded_df\n",
    "\n",
    "# Usage example\n",
    "csv_path = 'flickr_annotations_30k.csv'\n",
    "sp_model_path = 'spm.model'\n",
    "encoded_captions = encode_sentences(csv_path, sp_model_path) #Dimensions - batch size x num_images x num_captions x max caption length (after padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[19, 29, 373, 14, 2280, 124, 195, 20, 74, 177, 30, 360, 83, 7, 8, 538, 5],\n",
       " [19, 29, 15, 1113, 792, 17, 62, 95, 367, 1554, 5],\n",
       " [19, 40, 7, 55, 281, 17, 37, 7, 4, 538, 5],\n",
       " [6, 12, 7, 4, 32, 25, 37, 7, 4, 805, 5],\n",
       " [19, 555, 573, 653, 3793, 1927, 157, 5]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_captions.encoded_sentences[0] #encoding for 5 captions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_length):\n",
    "        super().__init__()\n",
    "        self.pos_encoding = self.create_pos_encoding(d_model, max_length)\n",
    "\n",
    "    def create_pos_encoding(self, d_model, max_length):\n",
    "        position = torch.arange(max_length).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_length, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        return nn.Parameter(pe, requires_grad=False)\n",
    "    \n",
    "    def forward(self, embeddings):\n",
    "        # Use embeddings to determine sequence length and to get device information\n",
    "        seq_len = embeddings.size(1)\n",
    "        pos_encoding = self.pos_encoding[:, :seq_len, :].to(embeddings.device)\n",
    "        return embeddings + pos_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedImageEncoder(nn.Module):\n",
    "    def __init__(self, num_patches, embed_dim=768, num_layers=6, num_heads=12, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.embed_dim = embed_dim\n",
    "        self.positional_encoding = PositionalEncoding(embed_dim, num_patches)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=d_ff, dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, flat_patches):\n",
    "        # Assume flat_patches is of shape [B, num_patches, embed_dim]\n",
    "        # where each patch is already flattened into a vector of embed_dim\n",
    "        x = flat_patches\n",
    "        \n",
    "        # You might need to adjust positional encodings if the sequence length\n",
    "        # could vary or if it's dynamically determined\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.transformer_encoder(x)  # Pass through transformer encoder\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA (NVIDIA GPU) is available, then use it; otherwise, fall back to CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# For PyTorch 1.12 or newer, to include support for Apple Silicon (MPS)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')  # Use Apple's Metal Performance Shaders (MPS)\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')  # Use NVIDIA GPU with CUDA\n",
    "else:\n",
    "    device = torch.device('cpu')  # Use CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Image Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ModifiedImageEncoder.__init__() got an unexpected keyword argument 'batch_first'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/harshil/Documents/MLX4/MLX4.6-MultiModalTransformers-CaptionsFromImages/MLX4.6/preproccessor.ipynb Cell 16\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harshil/Documents/MLX4/MLX4.6-MultiModalTransformers-CaptionsFromImages/MLX4.6/preproccessor.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m embed_dim \u001b[39m=\u001b[39m \u001b[39m768\u001b[39m  \u001b[39m# Assuming each patch is projected to an embedding dimension of 768\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harshil/Documents/MLX4/MLX4.6-MultiModalTransformers-CaptionsFromImages/MLX4.6/preproccessor.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Initialize the encoder\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/harshil/Documents/MLX4/MLX4.6-MultiModalTransformers-CaptionsFromImages/MLX4.6/preproccessor.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m image_encoder \u001b[39m=\u001b[39m ModifiedImageEncoder(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harshil/Documents/MLX4/MLX4.6-MultiModalTransformers-CaptionsFromImages/MLX4.6/preproccessor.ipynb#X26sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     num_patches\u001b[39m=\u001b[39;49mnum_patches,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harshil/Documents/MLX4/MLX4.6-MultiModalTransformers-CaptionsFromImages/MLX4.6/preproccessor.ipynb#X26sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     embed_dim\u001b[39m=\u001b[39;49membed_dim,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harshil/Documents/MLX4/MLX4.6-MultiModalTransformers-CaptionsFromImages/MLX4.6/preproccessor.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     num_layers\u001b[39m=\u001b[39;49m\u001b[39m6\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshil/Documents/MLX4/MLX4.6-MultiModalTransformers-CaptionsFromImages/MLX4.6/preproccessor.ipynb#X26sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     num_heads\u001b[39m=\u001b[39;49m\u001b[39m12\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshil/Documents/MLX4/MLX4.6-MultiModalTransformers-CaptionsFromImages/MLX4.6/preproccessor.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     d_ff\u001b[39m=\u001b[39;49m\u001b[39m2048\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshil/Documents/MLX4/MLX4.6-MultiModalTransformers-CaptionsFromImages/MLX4.6/preproccessor.ipynb#X26sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     dropout\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshil/Documents/MLX4/MLX4.6-MultiModalTransformers-CaptionsFromImages/MLX4.6/preproccessor.ipynb#X26sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     batch_first \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshil/Documents/MLX4/MLX4.6-MultiModalTransformers-CaptionsFromImages/MLX4.6/preproccessor.ipynb#X26sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshil/Documents/MLX4/MLX4.6-MultiModalTransformers-CaptionsFromImages/MLX4.6/preproccessor.ipynb#X26sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Assuming the device setup from the previous discussion\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshil/Documents/MLX4/MLX4.6-MultiModalTransformers-CaptionsFromImages/MLX4.6/preproccessor.ipynb#X26sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m image_encoder \u001b[39m=\u001b[39m image_encoder\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[0;31mTypeError\u001b[0m: ModifiedImageEncoder.__init__() got an unexpected keyword argument 'batch_first'"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "num_patches = (448 // 16) * (448 // 16)  # Assuming the image is resized to 448x448 and patches are 16x16\n",
    "embed_dim = 768  # Assuming each patch is projected to an embedding dimension of 768\n",
    "\n",
    "# Initialize the encoder\n",
    "image_encoder = ModifiedImageEncoder(\n",
    "    num_patches=num_patches,\n",
    "    embed_dim=embed_dim,\n",
    "    num_layers=6,\n",
    "    num_heads=12,\n",
    "    d_ff=2048,\n",
    "    dropout=0.1,\n",
    "    batch_first = True\n",
    ")\n",
    "\n",
    "# Assuming the device setup from the previous discussion\n",
    "image_encoder = image_encoder.to(device)\n",
    "patch_vectors_tensor = patch_vectors_tensor.to(device)\n",
    "\n",
    "# Encode the preprocessed patches\n",
    "with torch.no_grad():\n",
    "    encoded_patches = image_encoder(patch_vectors_tensor.unsqueeze(0))  # Add a batch dimension\n",
    "\n",
    "print(\"Encoded patches shape:\", encoded_patches.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
